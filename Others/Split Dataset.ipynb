{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Split Dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xrbFSw9Qp5FZ"},"source":["# Train & Validation\n","\n","vogliamo che la *distribuzione* di **foto per persona** e delle varie **classi** rimanga *uguale* sia nel training che nel validation set"]},{"cell_type":"markdown","metadata":{"id":"XYLvx8kHl3vY"},"source":["### Define function to Download  +  Split Train Set\n","\n","the first function download the dataset from gdrive\n","the second functions splits poeople's ids in training & validation"]},{"cell_type":"code","metadata":{"id":"x9gAUM6rpzyH"},"source":["import os, pandas as pd, numpy as np\n","from google_drive_downloader import GoogleDriveDownloader as gdd\n","from collections import defaultdict\n","\n","\n","def download_dataset():\n","  os.system('mkdir ./project_dataset')            # create folder for the dataset\n","  GDRIVE_ID = '1Re1N_Qc0884fnHfK0vFs5NvrMB5fK6lm' # id of file in google drive\n","  gdd.download_file_from_google_drive(file_id=GDRIVE_ID,  # function to download\n","                                    dest_path='./project_dataset/dataset.zip',\n","                                    unzip=True)\n","\n","\n","\n","def split_dataset(partitions=(.7,.3), force_at_least_one=False, rand_seed=0, verbose=False):\n","  \"\"\"\n","  returns a list of people's ids for every partition specified.\n","  eg. \n","  if there are 751 samples with partition (.5,.25,.25)\n","  the result will something like: [[370], [190], [190]]\n","  where [370] means an array containing 370 ids of people\n","  -----------------------\n","  we want to have at least 1 sample of every class in every partition \n","\n","  eg.\n","  input:  partitions=(.5, .5)\n","  output: [[ids of 376 people], [ids of other 376 people]]\n","  \"\"\"\n","  # check that the percentages of partitions sums to 1\n","  assert((sum(partitions)-1)**2 < 0.0002)\n","\n","  # get data\n","  download_dataset()\n","  df = pd.read_csv('./project_dataset/annotations_train.csv', index_col='id')\n","\n","  # process\n","  if len(partitions)==1: return df.index.to_list() ### SPECIAL CASE: no split is done\n","  splits = [[] for _ in range(len(partitions))]    # train, validation, (validation2)\n","\n","\n","  ###  sort by classes sizes, and then select samples staring from the smallest\n","  ###  in this way we are more sure to have at least one sample for every class\n","  keys = set(df.keys()) - {'id'}\n","  smallest_classes = []\n","  for column in keys:\n","    for value in df[column].unique():\n","      count = df.loc[df[column]==value].count()['age']    # 'age' do not matter, any field could do\n","      smallest_classes.append((count, column, value))\n","\n","  # smallest_classes = [(1, 'downpurple', 2),  (7, 'age', 1),  (8, 'downyellow', 2), ... ]\n","  smallest_classes.sort(key=lambda x:x[0])\n","\n","  for count, column, value in smallest_classes: \n","    # get specific combination category-value\n","    tmp  = df.loc[df[column]==value]           ### columns can be 'age', 'gender', ...         ### values can be 1, 2, ...\n","\n","    if count < len(partitions):\n","      # SPECIAL CASE: very very small class  --> eg. 'downpurple' has only 1 person\n","      # Risk of overfitting the purple guy instead of learning what is purple..... (hope in data augmentation)\n","      if verbose: print(f'- combination {column}={value} is very rare, copying this person in both splits')\n","      samples = tmp.index.to_list()\n","      for i, split in enumerate(splits):\n","        j = min(i, len(tmp)-1)\n","        split.append(samples[j])\n","      df = df.drop(samples)\n","      continue\n","    elif count < 30 and len(tmp) < len(partitions):\n","      # SPECIAL CASE: we are unlucky with the previous assignments\n","      # this depends on the correlation between small categories...\n","      # anyway, probably, this will never happen\n","      raise Exception('we are unlucky, try again') # do not happen with seeds 1 and 2\n","    elif len(tmp)==0:\n","      # SPECIAL CASE: we assigned everything already\n","      # so, the remaining classes are empty\n","      continue\n","\n","    if len(tmp) >= len(partitions) and force_at_least_one:\n","      # manually add 1 person of that category-value to each split\n","      samp = tmp.sample(n=len(partitions), random_state=rand_seed).index.tolist()\n","      for i, split in enumerate(splits):\n","        split.append(samp[i])\n","      tmp  = tmp.drop(samp)\n","      df = df.drop(samp)\n","\n","    # add the remaining samples randomly\n","    sections = [int(sum(partitions[:i])*len(tmp)) for i in range(1, len(partitions))] # if there are 100 samples with partition (.6,.2,.2) ==> [60, 80]\n","    samp2 = np.split(tmp.sample(frac=1, random_state=rand_seed).index.to_numpy(), sections)\n","    for i, split in enumerate(splits):\n","      split += list(samp2[i])\n","    df = df.drop(tmp.index.to_numpy())\n","\n","    if len(df) == 0:\n","      break\n","  return splits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"928NBHo2mLUy"},"source":["### Analysis of the Split Made By the Function in the Previous Cell"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSE9s41nlanv","executionInfo":{"status":"ok","timestamp":1616065626322,"user_tz":-60,"elapsed":1416,"user":{"displayName":"Thomas Reolon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAnqkSKz5M9Wf4oqAIzO8FYVAqIDiJwXWLocZr=s64","userId":"02614374704288605426"}},"outputId":"a2c4fbf2-dfe3-42bb-f9bd-c1a84805a114"},"source":["### TAKES THE RESULT OF SPLIT_DATASET AND RETURNS SOME INFOS ABOUT THE SPLIT ###\n","def print_split_stats(splits):\n","  \"\"\"\n","  util function that print information about the split.\n","  (I think that this function is consistent in creating good splits \n","  --> estimated generative pdf in training/validation seems similar)\n","  \"\"\"\n","  df = pd.read_csv('./project_dataset/annotations_train.csv', index_col='id')\n","  names = ['train', 'validation', 'validation2']\n","\n","  print(f'n_people = {len(df)}\\n------------------------------------------')\n","  counters = defaultdict(lambda:defaultdict(lambda:[0 for _ in range(len(splits))]))\n","  for i, split in enumerate(splits):\n","    print(f'len {names[i]} = {len(split)}')\n","    for pid in split:\n","      row = df.loc[pid]\n","      for k in row.keys():\n","        if k!='id':\n","          counters[k][int(row[k])][i] += 1\n","\n","  print('-----------------------------------------------')\n","  print('POSSIBLE VALUES FOR EACH COLUMN + [HOW MANY PEOPLE HAVE THAT VALUE]:')\n","  for k in df.keys():\n","    if k != 'id':\n","      possible_values = set(df[k].tolist())\n","      possible_values_and_counts = ', '.join([f'{val} {counters[k][int(val)]}'  for val in possible_values])\n","      percentage = counters[k][2] / (np.array(counters[k][2])+counters[k][1])\n","      print(k, ' '*(10-len(k)),':', possible_values_and_counts, '       \\033[94mprob_val_2=', percentage,'\\033[0m')\n","  print('\\033[94mNOTE: the values in blue should be pretty similar (distribution of classes in training/validation)\\033[0m')      \n","\n","  count_files  = [[] for _ in range(len(splits))]\n","  images = os.listdir('./project_dataset/train')\n","  splits = [set(s) for s in splits]\n","  for pid in df.index.to_list():\n","    for i, split in enumerate(splits):\n","      if pid in split:\n","        f_begin = ('0000'+str(pid)+'_')[-5:]\n","        images_about_pid = len([x for x in images if f_begin in x])\n","        count_files[i].append(images_about_pid)\n","  print('-----------------------------------------------')\n","  print('how many files each person has: \\n(good if the values in the arrays below seems to be generated by a similar pdf)')\n","  for cf in count_files:\n","    cf.sort()\n","    print(cf)\n","\n","### Print Stats\n","print_split_stats(split_dataset(verbose=True))\n","\n","\n","# What is the meaning of \"backpack    : 1 [386, 177], 2 [128, 61]        prob_val_2= [0.24902724 0.25630252]\"   ?\n","#     \n","# - \"backpack\" is the category\n","# - \"1 [386, 177]\" means that 387 people do not have a backpack in the training set and that 177 people do not have the bagpack in the validation set\n","# - \"2 [128, 61]\"  means that 128 people have a backpack in the training set and that 61 people have the bagpack in the validation set\n","# - \"prob_val_2= [0.24902724 0.25630252]\"   means that 24.902724% of the people in the training set have a bagpack (while 25.630252% have a bagpack in the validation set)\n","#\n","#  0.24902724 = 128/(128+386)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["- combination downpurple=2 is very rare, copying this person in both splits\n","n_people = 751\n","------------------------------------------\n","len train = 514\n","len validation = 238\n","-----------------------------------------------\n","POSSIBLE VALUES FOR EACH COLUMN + [HOW MANY PEOPLE HAVE THAT VALUE]:\n","age         : 1 [4, 3], 2 [420, 178], 3 [85, 54], 4 [5, 3]        \u001b[94mprob_val_2= [0.99056604 0.98342541] \u001b[0m\n","backpack    : 1 [386, 177], 2 [128, 61]        \u001b[94mprob_val_2= [0.24902724 0.25630252] \u001b[0m\n","bag         : 1 [380, 178], 2 [134, 60]        \u001b[94mprob_val_2= [0.26070039 0.25210084] \u001b[0m\n","handbag     : 1 [453, 218], 2 [61, 20]        \u001b[94mprob_val_2= [0.11867704 0.08403361] \u001b[0m\n","clothes     : 1 [69, 29], 2 [445, 209]        \u001b[94mprob_val_2= [0.86575875 0.87815126] \u001b[0m\n","down        : 1 [202, 87], 2 [312, 151]        \u001b[94mprob_val_2= [0.60700389 0.63445378] \u001b[0m\n","up          : 1 [30, 14], 2 [484, 224]        \u001b[94mprob_val_2= [0.94163424 0.94117647] \u001b[0m\n","hair        : 1 [331, 157], 2 [183, 81]        \u001b[94mprob_val_2= [0.35603113 0.34033613] \u001b[0m\n","hat         : 1 [499, 230], 2 [15, 8]        \u001b[94mprob_val_2= [0.02918288 0.03361345] \u001b[0m\n","gender      : 1 [285, 133], 2 [229, 105]        \u001b[94mprob_val_2= [0.44552529 0.44117647] \u001b[0m\n","upblack     : 1 [440, 197], 2 [74, 41]        \u001b[94mprob_val_2= [0.14396887 0.17226891] \u001b[0m\n","upwhite     : 1 [365, 169], 2 [149, 69]        \u001b[94mprob_val_2= [0.28988327 0.28991597] \u001b[0m\n","upred       : 1 [463, 216], 2 [51, 22]        \u001b[94mprob_val_2= [0.09922179 0.09243697] \u001b[0m\n","uppurple    : 1 [496, 229], 2 [18, 9]        \u001b[94mprob_val_2= [0.03501946 0.03781513] \u001b[0m\n","upyellow    : 1 [476, 221], 2 [38, 17]        \u001b[94mprob_val_2= [0.07392996 0.07142857] \u001b[0m\n","upgray      : 1 [453, 209], 2 [61, 29]        \u001b[94mprob_val_2= [0.11867704 0.12184874] \u001b[0m\n","upblue      : 1 [475, 222], 2 [39, 16]        \u001b[94mprob_val_2= [0.07587549 0.06722689] \u001b[0m\n","upgreen     : 1 [481, 224], 2 [33, 14]        \u001b[94mprob_val_2= [0.06420233 0.05882353] \u001b[0m\n","downblack   : 1 [316, 144], 2 [198, 94]        \u001b[94mprob_val_2= [0.38521401 0.39495798] \u001b[0m\n","downwhite   : 1 [480, 223], 2 [34, 15]        \u001b[94mprob_val_2= [0.06614786 0.06302521] \u001b[0m\n","downpink    : 1 [496, 228], 2 [18, 10]        \u001b[94mprob_val_2= [0.03501946 0.04201681] \u001b[0m\n","downpurple  : 1 [513, 237], 2 [1, 1]        \u001b[94mprob_val_2= [0.00194553 0.00420168] \u001b[0m\n","downyellow  : 1 [509, 235], 2 [5, 3]        \u001b[94mprob_val_2= [0.00972763 0.01260504] \u001b[0m\n","downgray    : 1 [437, 201], 2 [77, 37]        \u001b[94mprob_val_2= [0.14980545 0.15546218] \u001b[0m\n","downblue    : 1 [409, 199], 2 [105, 39]        \u001b[94mprob_val_2= [0.20428016 0.16386555] \u001b[0m\n","downgreen   : 1 [502, 232], 2 [12, 6]        \u001b[94mprob_val_2= [0.0233463  0.02521008] \u001b[0m\n","downbrown   : 1 [475, 218], 2 [39, 20]        \u001b[94mprob_val_2= [0.07587549 0.08403361] \u001b[0m\n","\u001b[94mNOTE: the values in blue should be pretty similar (distribution of classes in training/validation)\u001b[0m\n","-----------------------------------------------\n","how many files each person has: \n","(good if the values in the arrays below seems to be generated by a similar pdf)\n","[2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 35, 36, 36, 37, 37, 37, 38, 38, 38, 38, 39, 40, 40, 40, 41, 43, 44, 44, 46, 47, 48, 49, 52, 53, 54, 56, 58, 70, 72]\n","[2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 31, 33, 33, 33, 33, 34, 34, 34, 34, 37, 38, 39, 41, 42, 44, 47, 51, 52, 55, 59]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JOPqAESxmrpW"},"source":["### From list of people's ids to list of images"]},{"cell_type":"code","metadata":{"id":"WxMCKdK4DC7m"},"source":["def get_files(splits):\n","  \"\"\"\n","  for each list of people's ids, retrieve their files\n","  :return a list of filenames for each split\n","\n","  eg.\n","  input:  [[1,44], [6]]\n","  output: [['0001_c1_2131.jpg', '0001_c2_7134.jpg', '0044_c2_8364.jpg'], ['0006_c1_5341.jpg', '0006_c1_5373.jpg']]\n","  \"\"\"\n","  s_files  = [[] for _ in range(len(splits))]\n","  images = os.listdir('./project_dataset/train')\n","  splits = [set(s) for s in splits]\n","  for pid in set().union(*splits):\n","    for i, split in enumerate(splits):\n","      if pid in split:\n","        f_begin = ('0000'+str(pid)+'_')[-5:]\n","        s_files[i] += [x for x in images if f_begin in x]\n","  return s_files\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wg-Cyybsm4dY"},"source":["### Define Dataset\n","\n","define a class for our dataset"]},{"cell_type":"code","metadata":{"id":"9bhKWD3ym3Jz"},"source":["import torch, torchvision\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class ReIdentificationDataset(Dataset):\n","  \"\"\"\n","  this dataset receives in input the image_files that it must load and return\n","  when the image at index idx is asked, this Dataset checks if the pil image is already in the cache,\n","  if it is not, the file is loaded from the file system\n","  \"\"\"\n","  def __init__(self, root, files, transform=None, target_transform=None):\n","    super().__init__()\n","    self.root              = (root[-1]=='/') and root or (root+'/')          # add final slash if there isn't\n","    self.transform         = transform or torchvision.transforms.ToTensor()  # if not specified: transform to Tensor\n","    self.target_transform  = target_transform or (lambda y: torch.tensor(y)) # if not specified: transform to Tensor\n","\n","    self.files  = files\n","    self.df     = pd.read_csv('./project_dataset/annotations_train.csv', index_col='id')\n","    self._cache = [None for _ in range(len(files))]\n","\n","  def __getitem__(self, idx):\n","    # load image, target from file-system or from cache\n","    if self._cache[idx] is None:\n","      # load image\n","      path = self.root + self.files[idx]\n","      x = Image.open(path)\n","\n","      # load target\n","      pid = int(self.files[idx][:4])\n","      y = self.df.loc[pid].to_list()\n","      self._cache[idx] = (x,y)\n","    else:\n","      # retrieve from cache\n","      x,y = self._cache[idx]\n","\n","    x = self.transform(x)\n","    y = self.target_transform(y)\n","    return x, y\n","\n","  def __len__(self):\n","    return len(self.files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyuVHoR7nG4y"},"source":["### DataLoaders\n","\n","define a functions that puts toghether everything specified above and return Dataloaders for training and validation"]},{"cell_type":"code","metadata":{"id":"IA9G9qtZg44b"},"source":["from torch.utils.data import DataLoader\n","train_path='./project_dataset/train'\n","\n","def get_data_loaders(partitions, rand_seed=0, batch_size=16, transform=None, target_transform=None, pin_memory=False):\n","  \"\"\"\n","  This functions is a wrapper for the functions used above:\n","  0. download dataset if not already in memory (done inside split_dataset())\n","  1. ids of the people are splitted in training and validation set\n","  2. for each splits the files about the people in the split are retrieved\n","  3. a Dataset and a Dataloader are created\n","\n","  Parameters\n","  ----------\n","  partitions:         tuple\n","    percentages of how to split the dataset, eg. (0.7, 0.3) will return 2 dataloaders where the first will contain 70% of the people and the second 30%\n","    you can use more than 2 partitions,  eg. (.5, .25, .25) ---> train(50%), validation1(25%), validation2(25%)\n","\n","  rand_seed:          int (default=0)\n","    the seed used when doing shuffles  --> for deterministic results & reproducibility\n","\n","  batch_size:         int (default=16)\n","    how many images are returned in a batch\n","\n","  transform:          callable (default=None)\n","    rotations, cropping, transformations, normalizations, .... applied to the image\n","\n","  target_transform:   callable (default=None)\n","    transformations made to the target (eg. one-hot encoding for values in age; normalizing)\n","\n","  pin_memory:         bool (default=False)\n","    a speedup when using GPUs (automatic toDevice?)\n","\n","  Returns\n","  -------\n","  one DataLoader for each partition (usually 2, but if len(partition)==3 three DataLoaders are returned)\n","\n","  inputBatch= torch.Size([16, 3, 128, 64]), targetBatch=[16,30]\n","  \"\"\"\n","  # download dataset & split people's ids in train & validation\n","  splits = split_dataset(partitions, rand_seed=rand_seed) \n","\n","  # get the files \n","  splits = get_files(splits)\n","\n","  dataloaders = []\n","  for files in splits:\n","    # create dataset & dataloader for every split\n","    dataset = ReIdentificationDataset(root=train_path, files=files, transform=transform, target_transform=target_transform)\n","    d_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=pin_memory)\n","    dataloaders.append(d_loader)\n","\n","  return dataloaders"],"execution_count":null,"outputs":[]}]}